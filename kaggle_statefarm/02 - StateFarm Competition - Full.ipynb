{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# adds parent directory to python path so we can access code located there\n",
    "import os, sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path: sys.path.append(nb_dir)\n",
    "    \n",
    "# core imports\n",
    "from ohmeow_ml.keras_tf_util import *\n",
    "\n",
    "# configure matplotlib\n",
    "%matplotlib inline\n",
    "    \n",
    "# configure autoreload to re-load changed modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Define paths and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "DATA_HOME_DIR = current_dir + '/data/'\n",
    "DATA_CLASSES = [ dir for dir in os.listdir(DATA_HOME_DIR+'train') ]\n",
    "\n",
    "path = DATA_HOME_DIR\n",
    "# path = DATA_HOME_DIR + 'sample/'\n",
    "sample_path = DATA_HOME_DIR + 'sample/'\n",
    "\n",
    "train_path = path + 'train/'\n",
    "val_path = path + 'valid/'\n",
    "test_path = path + 'test/'\n",
    "\n",
    "models_path = path + 'models/'                      # save weights here\n",
    "results_path = path + 'results/'                    # save predictions here\n",
    "processed_data_path = path + 'preprocesed_data/'    # save preprocessed data used for training here\n",
    "\n",
    "if not os.path.exists(models_path): os.makedirs(models_path)\n",
    "if not os.path.exists(results_path): os.makedirs(results_path)\n",
    "if not os.path.exists(processed_data_path): os.makedirs(processed_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Preprocess the data\n",
    "\n",
    "We can save time by pre-processing the images (e.g., converting them to jpegs, resizing to 224x224) and saving them as a numpy array on the file system.  We can do the same for the train, validation, and test image class designations, filenames, and one-hot encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16951 images belonging to 10 classes.\n",
      "Found 5473 images belonging to 10 classes.\n",
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# get classes, one-hot encoded labels, and filenames\n",
    "train_classes, train_labels, train_filenames = get_batch_info(train_path)\n",
    "val_classes, val_labels, val_filenames = get_batch_info(val_path)\n",
    "test_filenames = get_batch_info(test_path)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data loaded ...\n",
      "validation data loaded ...\n"
     ]
    }
   ],
   "source": [
    " # get image data\n",
    "if not os.path.exists(processed_data_path+'train_data.bc'):\n",
    "    train_data = get_data(train_path)\n",
    "    save_array(processed_data_path+'train_data.bc', train_data)\n",
    "else:\n",
    "    train_data = load_array(processed_data_path+'train_data.bc')\n",
    "    print('training data loaded ...')\n",
    "\n",
    "if not os.path.exists(processed_data_path+'val_data.bc'):\n",
    "    val_data = get_data(val_path)\n",
    "    save_array(processed_data_path+'val_data.bc', val_data)\n",
    "else:\n",
    "    val_data = load_array(processed_data_path+'val_data.bc')\n",
    "    print('validation data loaded ...')\n",
    "\n",
    "# NOTE: with almost 80k records, trying to serialize the test set results in a memory error\n",
    "# if not os.path.exists(processed_data_path+'test_data.bc'):\n",
    "#     test_data = get_data(test_path)\n",
    "#     save_array(processed_data_path+'test_data.bc', test_data)\n",
    "# else:\n",
    "#     test_data = load_array(processed_data_path+'test_data.bc')\n",
    "#     print('test data loaded ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create training/validation batches and also define \"steps per epoch\" for each ... defines the # of batches per epoch (see `model.fit_generator()`).\n",
    "\n",
    "***ONLY RUN THIS CODE IF YOU NEED TO USE BATCHES INSTEAD OF PERSISTED IMAGE ARRAYS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# OPTION 1: BUILD BATCHES FROM FILE SYSTEM\n",
    "# train_batches = get_batches(train_path, batch_size=batch_size)\n",
    "# val_batches = get_batches(val_path, batch_size=batch_size*2, shuffle=False)\n",
    "\n",
    "# OPTION 2: BUILD BATCHES FROM IMAGE ARRAYS\n",
    "# gen = image.ImageDataGenerator()\n",
    "# train_batches = gen.flow(train_data, train_labels, batch_size=batch_size, shuffle=True)\n",
    "# val_batches = gen.flow(val_data, val_labels, batch_size=batch_size*2, shuffle=False)\n",
    "\n",
    "# DEFINE # OF STEPS TO TAKE IN FITTING BATCHES FOR BOTH TRAINING AND VALIDATION EXAMPLES\n",
    "# epoch_steps = math.ceil(train_batches.n/train_batches.batch_size)\n",
    "# val_steps = math.ceil(val_batches.n/val_batches.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple CNN\n",
    "\n",
    "2 conv layers with max pooling + a simple dense network is a good simple CNN to start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_cnn():\n",
    "    model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(224,224,3)),\n",
    "        Conv2D(32, (3,3), activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Flatten(),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limit_mem()\n",
    "model = simple_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16951 samples, validate on 5473 samples\n",
      "Epoch 1/2\n",
      "578s - loss: 0.6795 - acc: 0.8251 - val_loss: 1.9279 - val_acc: 0.3967\n",
      "Epoch 2/2\n",
      "573s - loss: 0.1113 - acc: 0.9874 - val_loss: 1.8846 - val_acc: 0.4550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f088affb400>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, train_labels, batch_size=batch_size, epochs=2, shuffle=True, \n",
    "          validation_data=(val_data, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16951 samples, validate on 5473 samples\n",
      "Epoch 1/5\n",
      "574s - loss: 0.0513 - acc: 0.9965 - val_loss: 1.8822 - val_acc: 0.4679\n",
      "Epoch 2/5\n",
      "573s - loss: 0.0303 - acc: 0.9989 - val_loss: 1.8765 - val_acc: 0.4650\n",
      "Epoch 3/5\n",
      "574s - loss: 0.0196 - acc: 0.9998 - val_loss: 1.8850 - val_acc: 0.4688\n",
      "Epoch 4/5\n",
      "574s - loss: 0.0141 - acc: 0.9998 - val_loss: 1.8204 - val_acc: 0.4836\n",
      "Epoch 5/5\n",
      "573s - loss: 0.0105 - acc: 0.9999 - val_loss: 1.7986 - val_acc: 0.4931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f088af47e48>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit(train_data, train_labels, batch_size=batch_size, epochs=5, shuffle=True, \n",
    "          validation_data=(val_data, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(models_path+'simple_cnn_weights.h5') # val_acc = 0.4931"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aug_val</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aug</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>channel_shift_range</th>\n",
       "      <td>10.00</td>\n",
       "      <td>0.464624</td>\n",
       "      <td>0.902444</td>\n",
       "      <td>1.735638</td>\n",
       "      <td>0.464444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height_shift_range</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.431795</td>\n",
       "      <td>0.917333</td>\n",
       "      <td>1.985304</td>\n",
       "      <td>0.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rotation_range</th>\n",
       "      <td>10.00</td>\n",
       "      <td>0.817478</td>\n",
       "      <td>0.770444</td>\n",
       "      <td>1.790981</td>\n",
       "      <td>0.476444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shear_range</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.431570</td>\n",
       "      <td>0.912667</td>\n",
       "      <td>1.761170</td>\n",
       "      <td>0.480444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>width_shift_range</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.867643</td>\n",
       "      <td>0.755333</td>\n",
       "      <td>1.771413</td>\n",
       "      <td>0.515556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoom_range</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.445235</td>\n",
       "      <td>0.899778</td>\n",
       "      <td>1.787687</td>\n",
       "      <td>0.477778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     aug_val  train_loss  train_acc  val_loss   val_acc\n",
       "aug                                                                    \n",
       "channel_shift_range    10.00    0.464624   0.902444  1.735638  0.464444\n",
       "height_shift_range      0.00    0.431795   0.917333  1.985304  0.436000\n",
       "rotation_range         10.00    0.817478   0.770444  1.790981  0.476444\n",
       "shear_range             0.00    0.431570   0.912667  1.761170  0.480444\n",
       "width_shift_range       0.05    0.867643   0.755333  1.771413  0.515556\n",
       "zoom_range              0.00    0.445235   0.899778  1.787687  0.477778"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best values\n",
    "df_augs = pd.read_csv(sample_path+'data_augmentation_results.csv')\n",
    "df_augs.sort_values('val_acc', ascending=False).groupby('aug').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_aug = image.ImageDataGenerator(channel_shift_range=10.0, height_shift_range=0.0, rotation_range=10.0, \n",
    "                                   shear_range=0.0, width_shift_range=0.05, zoom_range=0.0)\n",
    "\n",
    "aug_batches = gen_aug.flow(train_data, train_labels, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limit_mem()\n",
    "model = simple_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "577s - loss: 1.4254 - acc: 0.5438 - val_loss: 1.8220 - val_acc: 0.3768\n",
      "Epoch 2/2\n",
      "575s - loss: 0.5995 - acc: 0.8449 - val_loss: 1.7175 - val_acc: 0.4952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f08731d8630>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_steps = math.ceil(aug_batches.n/aug_batches.batch_size)\n",
    "model.fit_generator(aug_batches, epoch_steps, epochs=2, validation_data=(val_data, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "575s - loss: 0.3676 - acc: 0.9169 - val_loss: 1.6963 - val_acc: 0.5085\n",
      "Epoch 2/4\n",
      "574s - loss: 0.2698 - acc: 0.9427 - val_loss: 1.6049 - val_acc: 0.5363\n",
      "Epoch 3/4\n",
      "574s - loss: 0.2061 - acc: 0.9589 - val_loss: 1.5723 - val_acc: 0.5461\n",
      "Epoch 4/4\n",
      "575s - loss: 0.1687 - acc: 0.9680 - val_loss: 1.5861 - val_acc: 0.5407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f08731d85f8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(aug_batches, epoch_steps, epochs=4, validation_data=(val_data, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "577s - loss: 0.0623 - acc: 0.9901 - val_loss: 1.5507 - val_acc: 0.5620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f08722f2668>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(aug_batches, epoch_steps, epochs=4, validation_data=(val_data, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(models_path+'simple_cnn_da_weights.h5') # val_acc = 0.5620"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex CNN Architecutre\n",
    "\n",
    "We are adding in regularization via Dropout so this will work better on full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def complex_cnn(p_do=0.5, n_dense_outputs=256):\n",
    "    model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(224,224,3)),\n",
    "        Conv2D(32, (3,3), activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Conv2D(128, (3,3), activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((3,3)),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(n_dense_outputs, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p_do/4),\n",
    "        Dense(n_dense_outputs, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p_do),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limit_mem()\n",
    "model = complex_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_aug = image.ImageDataGenerator(channel_shift_range=10.0, height_shift_range=0.0, rotation_range=10.0, \n",
    "                                   shear_range=0.0, width_shift_range=0.05, zoom_range=0.0)\n",
    "\n",
    "aug_batches = gen_aug.flow(train_data, train_labels, batch_size=batch_size, shuffle=True)\n",
    "epoch_steps = math.ceil(aug_batches.n/aug_batches.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "636s - loss: 1.7451 - acc: 0.4699 - val_loss: 2.1765 - val_acc: 0.3170\n",
      "Epoch 2/2\n",
      "634s - loss: 0.6807 - acc: 0.7763 - val_loss: 1.7729 - val_acc: 0.5081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0858ba55f8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(aug_batches, epoch_steps, epochs=2, validation_data=(val_data, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "633s - loss: 0.4139 - acc: 0.8669 - val_loss: 1.6830 - val_acc: 0.5695\n",
      "Epoch 2/4\n",
      "634s - loss: 0.2573 - acc: 0.9206 - val_loss: 1.4854 - val_acc: 0.6141\n",
      "Epoch 3/4\n",
      "633s - loss: 0.1832 - acc: 0.9451 - val_loss: 1.4465 - val_acc: 0.6386\n",
      "Epoch 4/4\n",
      "634s - loss: 0.1550 - acc: 0.9540 - val_loss: 1.6164 - val_acc: 0.5752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f083394be10>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(aug_batches, epoch_steps, epochs=4, validation_data=(val_data, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "633s - loss: 0.0887 - acc: 0.9746 - val_loss: 1.6451 - val_acc: 0.6464\n",
      "Epoch 2/4\n",
      "632s - loss: 0.0756 - acc: 0.9782 - val_loss: 1.5715 - val_acc: 0.6521\n",
      "Epoch 3/4\n",
      "632s - loss: 0.0657 - acc: 0.9810 - val_loss: 1.7755 - val_acc: 0.6168\n",
      "Epoch 4/4\n",
      "633s - loss: 0.0628 - acc: 0.9809 - val_loss: 1.7813 - val_acc: 0.6366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f083395d978>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(aug_batches, epoch_steps, epochs=6, validation_data=(val_data, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "633s - loss: 0.0380 - acc: 0.9887 - val_loss: 1.7406 - val_acc: 0.5978\n",
      "Epoch 2/4\n",
      "633s - loss: 0.0387 - acc: 0.9887 - val_loss: 1.5417 - val_acc: 0.6592\n",
      "Epoch 3/4\n",
      "633s - loss: 0.0309 - acc: 0.9906 - val_loss: 1.8036 - val_acc: 0.6338\n",
      "Epoch 4/4\n",
      "631s - loss: 0.0340 - acc: 0.9906 - val_loss: 1.8142 - val_acc: 0.6203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f083395dc88>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 0.00001\n",
    "model.fit_generator(aug_batches, epoch_steps, epochs=8, validation_data=(val_data, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(models_path+'complex_cnn_da_weights.h5') # val_acc = ~0.63"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-compute output from various layers to use as input in various experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Pre-compute output from VGG's 2nd to last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limit_mem()\n",
    "model = VGG19(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pop last layer and set model.outputs = to that of the now last layer\n",
    "model.layers.pop()\n",
    "\n",
    "# model.layers[-1].outbound_nodes = [] ... this is not needed\n",
    "model.outputs = [model.layers[-1].output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(processed_data_path+'train_features_ft_2nd_to_ll.bc'):\n",
    "    train_features_ft = model.predict(train_data, 4)\n",
    "    val_features_ft = model.predict(val_data, 4)\n",
    "    \n",
    "    save_array(processed_data_path+'train_features_ft_2nd_to_ll.bc', train_features_ft)\n",
    "    save_array(processed_data_path+'val_features_ft_2nd_to_ll.bc', val_features_ft)\n",
    "else:\n",
    "    train_features_ft = load_array(processed_data_path+'train_features_ft_2nd_to_ll.bc')\n",
    "    val_features_ft = load_array(processed_data_path+'val_features_ft_2nd_to_ll.bc')\n",
    "    \n",
    "print(train_features_ft.shape)\n",
    "print(val_features_ft.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Pre-compute output from convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limit_mem()\n",
    "model = VGG19(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(processed_data_path+'train_features_ft_conv.bc'):\n",
    "    train_features_ft = model.predict(train_data, 4)\n",
    "    val_features_ft = model.predict(val_data, 4)\n",
    "    \n",
    "    save_array(processed_data_path+'train_features_ft_conv.bc', train_features_ft)\n",
    "    save_array(processed_data_path+'val_features_ft_conv.bc', val_features_ft)\n",
    "else:\n",
    "    train_features_ft = load_array(processed_data_path+'train_features_ft_conv.bc')\n",
    "    val_features_ft = load_array(processed_data_path+'val_features_ft_conv.bc')\n",
    "    \n",
    "print(train_features_ft.shape)\n",
    "print(val_features_ft.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train a linear classifier using the pre-computed output from 2nd to last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limit_mem()\n",
    "model = VGG19(include_top=True, weights='imagenet')\n",
    "model.layers.pop()\n",
    "model.outputs = [model.layers[-1].output]\n",
    "\n",
    "train_features_ft = load_array(processed_data_path+'train_features_ft_2nd_to_ll.bc')\n",
    "val_features_ft = load_array(processed_data_path+'val_features_ft_2nd_to_ll.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lm_from_vgg_2ll():\n",
    "    m = Sequential([\n",
    "        Dense(10, activation='softmax', input_shape = model.layers[-1].output_shape[1:])\n",
    "    ])\n",
    "    \n",
    "    m.compile(optimizer=Adam(lr=1e-05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm = build_lm_from_vgg_2ll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm.fit(train_features_ft, train_labels, batch_size=batch_size, epochs=12, shuffle=True,\n",
    "       validation_data=(val_features_ft, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm.optimizer.lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Train model after replacing last layer with a Dense layer having 10 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limit_mem()\n",
    "model = VGG19(weights='imagenet', include_top=True)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = finetune(model, 10)\n",
    "# model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
