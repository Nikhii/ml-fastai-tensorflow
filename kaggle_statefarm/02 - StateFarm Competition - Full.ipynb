{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# adds parent directory to python path so we can access code located there\n",
    "import os, sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path: sys.path.append(nb_dir)\n",
    "    \n",
    "# core imports\n",
    "from ohmeow_ml.keras_tf_util import *\n",
    "\n",
    "# other imports\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# configure matplotlib\n",
    "%matplotlib inline\n",
    "    \n",
    "# configure autoreload to re-load changed modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Define paths and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "DATA_HOME_DIR = current_dir + '/data/'\n",
    "DATA_CLASSES = [ dir for dir in os.listdir(DATA_HOME_DIR+'train') ]\n",
    "\n",
    "path = DATA_HOME_DIR\n",
    "# path = DATA_HOME_DIR + 'sample/'\n",
    "sample_path = DATA_HOME_DIR + 'sample/'\n",
    "\n",
    "train_path = path + 'train/'\n",
    "val_path = path + 'valid/'\n",
    "test_path = path + 'test/'\n",
    "\n",
    "models_path = path + 'models/'                      # save weights here\n",
    "results_path = path + 'results/'                    # save predictions here\n",
    "processed_data_path = path + 'preprocesed_data/'    # save preprocessed data used for training here\n",
    "\n",
    "if not os.path.exists(models_path): os.makedirs(models_path)\n",
    "if not os.path.exists(results_path): os.makedirs(results_path)\n",
    "if not os.path.exists(processed_data_path): os.makedirs(processed_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "# batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def do_clip(arr, mx):\n",
    "    return np.clip(arr, (1-mx)/9, mx)\n",
    "\n",
    "def create_submission(preds, filename='subm.gz'):\n",
    "    subm = do_clip(preds, 0.93)\n",
    "    subm_file = results_path+filename\n",
    "    \n",
    "    batches = get_batches(train_path, batch_size=1, shuffle=False)\n",
    "    classes = sorted(batches.class_indices, key=batches.class_indices.get)\n",
    "    \n",
    "    df_subm = pd.DataFrame(subm, columns=classes)\n",
    "    df_subm.insert(0, 'img', [a[8:] for a in test_filenames])\n",
    "    #print(df_subm.head())\n",
    "    \n",
    "    df_subm.to_csv(subm_file, index=False, compression='gzip')\n",
    "    return subm_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Preprocess the data\n",
    "\n",
    "We can save time by pre-processing the images (e.g., converting them to jpegs, resizing to 224x224) and saving them as a numpy array on the file system.  We can do the same for the train, validation, and test image class designations, filenames, and one-hot encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16951 images belonging to 10 classes.\n",
      "Found 5473 images belonging to 10 classes.\n",
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# get classes, one-hot encoded labels, and filenames\n",
    "train_classes, train_labels, train_filenames = get_batch_info(train_path)\n",
    "val_classes, val_labels, val_filenames = get_batch_info(val_path)\n",
    "test_filenames = get_batch_info(test_path)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    " # get image data\n",
    "if not os.path.exists(processed_data_path+'train_data.bc'):\n",
    "    train_data = get_data(train_path)\n",
    "    save_array(processed_data_path+'train_data.bc', train_data)\n",
    "else:\n",
    "    train_data = load_array(processed_data_path+'train_data.bc')\n",
    "    print('training data loaded ...')\n",
    "\n",
    "if not os.path.exists(processed_data_path+'val_data.bc'):\n",
    "    val_data = get_data(val_path)\n",
    "    save_array(processed_data_path+'val_data.bc', val_data)\n",
    "else:\n",
    "    val_data = load_array(processed_data_path+'val_data.bc')\n",
    "    print('validation data loaded ...')\n",
    "\n",
    "# NOTE: with almost 80k records, trying to serialize the test set results in a memory error\n",
    "# if not os.path.exists(processed_data_path+'test_data.bc'):\n",
    "#     test_data = get_data(test_path)\n",
    "#     save_array(processed_data_path+'test_data.bc', test_data)\n",
    "# else:\n",
    "#     test_data = load_array(processed_data_path+'test_data.bc')\n",
    "#     print('test data loaded ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create training/validation batches and also define \"steps per epoch\" for each ... defines the # of batches per epoch (see `model.fit_generator()`).\n",
    "\n",
    "***ONLY RUN THIS CODE IF YOU NEED TO USE BATCHES INSTEAD OF PERSISTED IMAGE ARRAYS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# OPTION 1: BUILD BATCHES FROM FILE SYSTEM\n",
    "# train_batches = get_batches(train_path, batch_size=batch_size)\n",
    "# val_batches = get_batches(val_path, batch_size=batch_size*2, shuffle=False)\n",
    "\n",
    "# OPTION 2: BUILD BATCHES FROM IMAGE ARRAYS\n",
    "# gen = image.ImageDataGenerator()\n",
    "# train_batches = gen.flow(train_data, train_labels, batch_size=batch_size, shuffle=True)\n",
    "# val_batches = gen.flow(val_data, val_labels, batch_size=batch_size*2, shuffle=False)\n",
    "\n",
    "# DEFINE # OF STEPS TO TAKE IN FITTING BATCHES FOR BOTH TRAINING AND VALIDATION EXAMPLES\n",
    "# epoch_steps = math.ceil(train_batches.n/train_batches.batch_size)\n",
    "# val_steps = math.ceil(val_batches.n/val_batches.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Simple CNN\n",
    "\n",
    "2 conv layers with max pooling + a simple dense network is a good simple CNN to start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def simple_cnn():\n",
    "    model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(224,224,3)),\n",
    "        Conv2D(32, (3,3), activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Flatten(),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Without Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "limit_mem()\n",
    "model = simple_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_data, train_labels, batch_size=batch_size, epochs=2, shuffle=True, \n",
    "          validation_data=(val_data, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit(train_data, train_labels, batch_size=batch_size, epochs=5, shuffle=True, \n",
    "          validation_data=(val_data, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(models_path+'simple_cnn_weights.h5') # val_acc = 0.4931"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### With Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get the best values\n",
    "df_augs = pd.read_csv(sample_path+'data_augmentation_results.csv')\n",
    "df_augs.sort_values('val_acc', ascending=False).groupby('aug').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gen_aug = image.ImageDataGenerator(channel_shift_range=10.0, height_shift_range=0.0, rotation_range=10.0, \n",
    "                                   shear_range=0.0, width_shift_range=0.05, zoom_range=0.0)\n",
    "\n",
    "aug_batches = gen_aug.flow(train_data, train_labels, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "limit_mem()\n",
    "model = simple_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "epoch_steps = math.ceil(aug_batches.n/aug_batches.batch_size)\n",
    "model.fit_generator(aug_batches, epoch_steps, epochs=2, validation_data=(val_data, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(aug_batches, epoch_steps, epochs=4, validation_data=(val_data, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(aug_batches, epoch_steps, epochs=4, validation_data=(val_data, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(models_path+'simple_cnn_da_weights.h5') # val_acc = 0.5620"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Complex CNN Architecutre\n",
    "\n",
    "We are adding in regularization via Dropout so this will work better on full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def complex_cnn(p_do=0.5, n_dense_outputs=256):\n",
    "    model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(224,224,3)),\n",
    "        Conv2D(32, (3,3), activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((3,3)),\n",
    "        Conv2D(128, (3,3), activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((3,3)),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(n_dense_outputs, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p_do/4),\n",
    "        Dense(n_dense_outputs, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p_do),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "limit_mem()\n",
    "model = complex_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gen_aug = image.ImageDataGenerator(channel_shift_range=10.0, height_shift_range=0.0, rotation_range=10.0, \n",
    "                                   shear_range=0.0, width_shift_range=0.05, zoom_range=0.0)\n",
    "\n",
    "aug_batches = gen_aug.flow(train_data, train_labels, batch_size=batch_size, shuffle=True)\n",
    "epoch_steps = math.ceil(aug_batches.n/aug_batches.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(aug_batches, epoch_steps, epochs=2, validation_data=(val_data, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(aug_batches, epoch_steps, epochs=4, validation_data=(val_data, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(aug_batches, epoch_steps, epochs=6, validation_data=(val_data, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.00001\n",
    "model.fit_generator(aug_batches, epoch_steps, epochs=8, validation_data=(val_data, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(models_path+'complex_cnn_da_weights.h5') # val_acc = ~0.63"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune Pre-Trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train a linear classifier using the pre-computed output from 2nd to last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limit_mem()\n",
    "model = VGG19(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pop last layer and set model.outputs = to that of the now last layer\n",
    "model.layers.pop()\n",
    "\n",
    "# model.layers[-1].outbound_nodes = [] ... this is not needed\n",
    "model.outputs = [model.layers[-1].output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Pre-compute output for train, validation, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# A. precompute the 2nd to last layer for training and validation data sets\n",
    "if not os.path.exists(processed_data_path+'train_features_ll.bc'):\n",
    "    train_features_ll = model.predict(train_data, 4)\n",
    "    val_features_ll = model.predict(val_data, 4)\n",
    "    \n",
    "    save_array(processed_data_path+'train_features_ll.bc', train_features_ll)\n",
    "    save_array(processed_data_path+'val_features_ll.bc', val_features_ll)\n",
    "else:\n",
    "    train_features_ll = load_array(processed_data_path+'train_features_ll.bc')\n",
    "    val_features_ll = load_array(processed_data_path+'val_features_ll.bc')\n",
    "    \n",
    "print('training data:', train_features_ll.shape)\n",
    "print('validation data:', val_features_ll.shape)\n",
    "\n",
    "# B. do the same for augmented training data ... make this 5-10x larger\n",
    "if not os.path.exists(processed_data_path+'da_train_features_ll.bc'):\n",
    "    da_gen = image.ImageDataGenerator(channel_shift_range=10.0, height_shift_range=0.0, rotation_range=10.0, \n",
    "                                   shear_range=0.0, width_shift_range=0.05, zoom_range=0.0)\n",
    "\n",
    "    # shuffle=False because we are going to have to add labels later for however many \n",
    "    # augmented sets of the training data\n",
    "    da_batches = get_batches(train_path, da_gen, batch_size=4, shuffle=False) \n",
    "    da_train_features_ll = np.concatenate(\n",
    "        [ model.predict_generator(da_batches, (da_batches.n/da_batches.batch_size), verbose=2) for i in range(5)])\n",
    "    \n",
    "    save_array(processed_data_path+'da_train_features_ll.bc', da_train_features_ll)\n",
    "else:\n",
    "    da_train_features_ll = load_array(processed_data_path+'da_train_features_ll.bc')\n",
    "    \n",
    "print('augmented data:', da_train_features_ll.shape)\n",
    "\n",
    "# C. do the same for test data\n",
    "if not os.path.exists(processed_data_path+'test_features_ll.bc'):\n",
    "    test_batches = get_batches(test_path, batch_size=4, shuffle=False)\n",
    "    test_features_ll = model.predict_generator(test_batches, (test_batches.n/test_batches.batch_size), verbose=2)\n",
    "    save_array(processed_data_path+'test_features_ll.bc', test_features_ll)\n",
    "else:\n",
    "    test_features_ll = load_array(processed_data_path+'test_features_ll.bc')\n",
    "    \n",
    "print('test data:', test_features_ll.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_train_features_ll = np.concatenate([da_train_features_ll, train_features_ll])\n",
    "all_train_labels_ll = np.concatenate([train_labels]*6)\n",
    "\n",
    "print('all training features shape:', all_train_features_ll.shape)\n",
    "print('all training labels shape:', all_train_labels_ll.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers: layer.trainable = False\n",
    "ft_ll_model = Sequential([ Dense(10, activation='softmax', input_shape=model.layers[-1].output_shape[1:]) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ft_ll_model.compile(optimizer=Adam(lr=1e-05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "ft_ll_model.fit(all_train_features_ll, all_train_labels_ll, batch_size=batch_size, epochs=15, \n",
    "          validation_data=(val_features_ll, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ft_ll_model.optimizer.lr = 0.001\n",
    "ft_ll_model.fit(all_train_features_ll, all_train_labels_ll, batch_size=batch_size, epochs=5, \n",
    "          validation_data=(val_features_ll, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ft_ll_model.save_weights(models_path+'ft_ll_model_weights.h5') # val_acc = 0.33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Precompute the convolutional and use in FC NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limit_mem()\n",
    "model = VGG19(include_top=False, weights='imagenet', input_shape=(224,224,3)) # must include input_shape if include_top=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Precompute output for train, validation, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: (16951, 7, 7, 512)\n",
      "validation data: (5473, 7, 7, 512)\n",
      "augmented data: (84755, 7, 7, 512)\n",
      "test data: (79726, 7, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "# A. precompute the 2nd to last layer for training and validation data sets\n",
    "if not os.path.exists(processed_data_path+'train_features_conv.bc'):\n",
    "    train_features_conv = model.predict(train_data, 4)\n",
    "    val_features_conv = model.predict(val_data, 4)\n",
    "    \n",
    "    save_array(processed_data_path+'train_features_conv.bc', train_features_conv)\n",
    "    save_array(processed_data_path+'val_features_conv.bc', val_features_conv)\n",
    "else:\n",
    "    train_features_conv = load_array(processed_data_path+'train_features_conv.bc')\n",
    "    val_features_conv = load_array(processed_data_path+'val_features_conv.bc')\n",
    "    \n",
    "print('training data:', train_features_conv.shape)\n",
    "print('validation data:', val_features_conv.shape)\n",
    "\n",
    "# B. do the same for augmented training data ... make this 5-10x larger\n",
    "if not os.path.exists(processed_data_path+'da_train_features_conv.bc'):\n",
    "    da_gen = image.ImageDataGenerator(channel_shift_range=10.0, height_shift_range=0.0, rotation_range=10.0, \n",
    "                                   shear_range=0.0, width_shift_range=0.05, zoom_range=0.0)\n",
    "\n",
    "    # shuffle=False because we are going to have to add labels later for however many \n",
    "    # augmented sets of the training data\n",
    "    da_batches_conv = get_batches(train_path, da_gen, batch_size=4, shuffle=False) \n",
    "    da_train_features_conv = np.concatenate(\n",
    "        [ model.predict_generator(da_batches_conv, (da_batches_conv.n/da_batches_conv.batch_size), verbose=2) for i in range(5) ])\n",
    "    \n",
    "    save_array(processed_data_path+'da_train_features_conv.bc', da_train_features_conv)\n",
    "else:\n",
    "    da_train_features_conv = load_array(processed_data_path+'da_train_features_conv.bc')\n",
    "    \n",
    "print('augmented data:', da_train_features_conv.shape)\n",
    "\n",
    "# C. do the same for test data\n",
    "if not os.path.exists(processed_data_path+'test_features_conv.bc'):\n",
    "    test_batches_conv = get_batches(test_path, batch_size=4, shuffle=False)\n",
    "    test_features_conv = model.predict_generator(test_batches_conv, (test_batches_conv.n/test_batches_conv.batch_size), verbose=2)\n",
    "    save_array(processed_data_path+'test_features_conv.bc', test_features_conv)\n",
    "else:\n",
    "    test_features_conv = load_array(processed_data_path+'test_features_conv.bc')\n",
    "    \n",
    "print('test data:', test_features_conv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all training features shape: (101706, 7, 7, 512)\n",
      "all training labels shape: (101706, 10)\n"
     ]
    }
   ],
   "source": [
    "all_train_features_conv = np.concatenate([da_train_features_conv, train_features_conv])\n",
    "all_train_labels_conv = np.concatenate([train_labels]*6)\n",
    "\n",
    "print('all training features shape:', all_train_features_conv.shape)\n",
    "print('all training labels shape:', all_train_labels_conv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_fc_layers(dropout_p=0.5, dense_output=256):\n",
    "    return [\n",
    "        Flatten(input_shape=model.layers[-1].output_shape[1:]),\n",
    "        Dropout(dropout_p),\n",
    "        Dense(dense_output, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_p),\n",
    "        Dense(dense_output, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_p),\n",
    "        Dense(10, activation='softmax')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fc_model = Sequential(build_fc_layers(0.6, 512))\n",
    "fc_model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fc_model.fit(all_train_features_conv, all_train_labels_conv, batch_size=batch_size, epochs=2,\n",
    "            validation_data=(val_features_conv, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fc_model.optimizer.lr = 0.01\n",
    "fc_model.fit(all_train_features_conv, all_train_labels_conv, batch_size=batch_size, epochs=4,\n",
    "            validation_data=(val_features_conv, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fc_model.optimizer.lr = 0.001\n",
    "fc_model.fit(all_train_features_conv, all_train_labels_conv, batch_size=batch_size, epochs=4,\n",
    "            validation_data=(val_features_conv, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fc_model.optimizer.lr = 0.0001\n",
    "fc_model.fit(all_train_features_conv, all_train_labels_conv, batch_size=batch_size, epochs=4,\n",
    "            validation_data=(val_features_conv, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fc_model.save_weights(models_path+'ft_fc_model_weights_do-pt6_d-512.h5') # val_acc = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Evaluate and create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fc_model.evaluate(val_features_conv, val_labels, batch_size=batch_size, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds = fc_model.predict(test_features_conv, batch_size=batch_size, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16951 images belonging to 10 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='/home/ubuntu/ml-fastai-tensorflow/kaggle_statefarm/data/results/ft_fc_model_subm01.gz' target='_blank'>/home/ubuntu/ml-fastai-tensorflow/kaggle_statefarm/data/results/ft_fc_model_subm01.gz</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/ml-fastai-tensorflow/kaggle_statefarm/data/results/ft_fc_model_subm01.gz"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm_file = create_submission(preds, 'ft_fc_model_subm01.gz')\n",
    "FileLink(subm_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Add pseudo-labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_mem()\n",
    "fc_model = Sequential(build_fc_layers(0.6, 512))\n",
    "fc_model.load_weights(models_path+'ft_fc_model_weights_do-pt6_d-512.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_val_labels = fc_model.predict(val_features_conv, batch_size=4, verbose=2)\n",
    "pseudo_test_labels = fc_model.predict(test_features_conv, batch_size=4, verbose=2)\n",
    "\n",
    "combo_train_val_labels = np.concatenate([pseudo_val_labels, all_train_labels_conv])\n",
    "combo_train_val_feat = np.concatenate([val_features_conv, all_train_features_conv])\n",
    "\n",
    "combo_all_labels = np.concatenate([pseudo_test_labels, combo_train_val_labels])\n",
    "combo_all_feat = np.concatenate([test_features_conv, combo_train_val_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc_model = Sequential(build_fc_layers(0.6, 512))\n",
    "fc_model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 186905 samples, validate on 5473 samples\n",
      "Epoch 1/2\n",
      "83s - loss: 0.6293 - acc: 0.8384 - val_loss: 0.7780 - val_acc: 0.7818\n",
      "Epoch 2/2\n",
      "83s - loss: 0.4503 - acc: 0.8920 - val_loss: 0.8102 - val_acc: 0.7711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc7a62f3860>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.fit(combo_all_feat, combo_all_labels, batch_size=batch_size, epochs=2,\n",
    "            validation_data=(val_features_conv, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 186905 samples, validate on 5473 samples\n",
      "Epoch 1/4\n",
      "83s - loss: 0.4250 - acc: 0.8992 - val_loss: 0.8661 - val_acc: 0.7749\n",
      "Epoch 2/4\n",
      "83s - loss: 0.4076 - acc: 0.9036 - val_loss: 0.8042 - val_acc: 0.7780\n",
      "Epoch 3/4\n",
      "83s - loss: 0.3945 - acc: 0.9080 - val_loss: 0.8524 - val_acc: 0.7687\n",
      "Epoch 4/4\n",
      "82s - loss: 0.3845 - acc: 0.9116 - val_loss: 0.8475 - val_acc: 0.7680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc7a75cbcf8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.optimizer.lr = 0.01\n",
    "fc_model.fit(combo_all_feat, combo_all_labels, batch_size=batch_size, epochs=4,\n",
    "            validation_data=(val_features_conv, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 186905 samples, validate on 5473 samples\n",
      "Epoch 1/4\n",
      "83s - loss: 0.3748 - acc: 0.9139 - val_loss: 0.8413 - val_acc: 0.7696\n",
      "Epoch 2/4\n",
      "83s - loss: 0.3702 - acc: 0.9150 - val_loss: 0.8931 - val_acc: 0.7570\n",
      "Epoch 3/4\n",
      "83s - loss: 0.3637 - acc: 0.9168 - val_loss: 0.8303 - val_acc: 0.7749\n",
      "Epoch 4/4\n",
      "83s - loss: 0.3599 - acc: 0.9181 - val_loss: 0.8637 - val_acc: 0.7736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc7a6171b70>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.optimizer.lr = 0.001\n",
    "fc_model.fit(combo_all_feat, combo_all_labels, batch_size=batch_size, epochs=4,\n",
    "            validation_data=(val_features_conv, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 186905 samples, validate on 5473 samples\n",
      "Epoch 1/4\n",
      "83s - loss: 0.3571 - acc: 0.9186 - val_loss: 0.8393 - val_acc: 0.7680\n",
      "Epoch 2/4\n",
      "83s - loss: 0.3533 - acc: 0.9195 - val_loss: 0.8596 - val_acc: 0.7654\n",
      "Epoch 3/4\n",
      "83s - loss: 0.3497 - acc: 0.9207 - val_loss: 0.8876 - val_acc: 0.7700\n",
      "Epoch 4/4\n",
      "83s - loss: 0.3464 - acc: 0.9215 - val_loss: 0.8367 - val_acc: 0.7707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc7a60da470>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.optimizer.lr = 0.0001\n",
    "fc_model.fit(combo_all_feat, combo_all_labels, batch_size=batch_size, epochs=4,\n",
    "            validation_data=(val_features_conv, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
